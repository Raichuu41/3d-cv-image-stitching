{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "from matplotlib.transforms import Affine2D\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import skimage.exposure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import rotate\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(imgs, size=(10,6), title=\"\"):\n",
    "    fig, axes = plt.subplots(1,len(imgs), figsize=size)\n",
    "    for i in range(len(imgs)):\n",
    "        axes[i].imshow(imgs[i])\n",
    "\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path: str, appear=False, show_size=(12,6), title = \"\"):\n",
    "    image_paths = sorted(glob.glob(path))\n",
    "    imgs = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        imgs.append(img)\n",
    "    \n",
    "    if appear == True:\n",
    "        show_imgs(imgs, show_size, title)\n",
    "\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nomalize_images(imgs, appear=False, title=\"\"):\n",
    "\n",
    "    nomal_imgs = []\n",
    "    for img in imgs:\n",
    "        img_norm = img / np.max(img)\n",
    "        img_norm_uint8 = (img_norm * 255).astype(np.uint8)\n",
    "\n",
    "        nomal_imgs.append(img_norm_uint8)\n",
    "\n",
    "    if appear == True:\n",
    "        show_imgs(nomal_imgs, title=title)\n",
    "\n",
    "    return nomal_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gray_images(imgs, appear=False, title=\"\"):\n",
    "    \n",
    "    gray_imgs = []\n",
    "    for img in imgs:\n",
    "        \n",
    "        if img.shape == 3:\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            gray_imgs.append(gray_img)\n",
    "        else:\n",
    "            gray_imgs.append(img)\n",
    "\n",
    "    if appear == True:\n",
    "        show_imgs(gray_imgs, title=title)\n",
    "\n",
    "    return gray_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_coner_detector(imgs: list, blockSize: int, ksize: int, k_harris_parameter=0.04, threshold=0.01, appear=False):\n",
    "\n",
    "    keypoints = []\n",
    "    for img in imgs:\n",
    "        conner = cv2.cornerHarris(img, blockSize, ksize, k_harris_parameter)\n",
    "        conner = cv2.dilate(conner, None)\n",
    "        \n",
    "        points = np.argwhere(conner > threshold * conner.max())\n",
    "        keypoint = [cv2.KeyPoint(float(x[1]), float(x[0]), 1) for x in points]\n",
    "\n",
    "        keypoints.append(keypoint)\n",
    "\n",
    "    if appear == True:\n",
    "        plot_keypoints(imgs, keypoints)\n",
    "\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints(imgs, keypoints):\n",
    "\n",
    "    _, axes = plt.subplots(1,len(imgs), figsize=(10,6))\n",
    "    for i in range(len(imgs)):\n",
    "        points = np.array([list(keypoint.pt) for keypoint in keypoints[i]])\n",
    "\n",
    "        axes[i].imshow(imgs[i])\n",
    "        axes[i].scatter(points[:,0], points[:,1], marker='x', color='r')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_images(gray_imgs, keypoints, num_top_matches=50, appear=False):\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    #src image\n",
    "    gray_src = gray_imgs[0]\n",
    "    src_keypoints = keypoints[0]\n",
    "\n",
    "    # des image\n",
    "    gray_des = gray_imgs[1]\n",
    "    des_keypoints = keypoints[1]\n",
    "\n",
    "    # Detect descriptors using SIFT\n",
    "    src_keypoints, src_descriptors = sift.compute(gray_src, src_keypoints)\n",
    "    des_keypoints, des_descriptors = sift.compute(gray_des, des_keypoints)\n",
    "    \n",
    "    # Match descriptors using FLANN matcher\n",
    "    matcher = cv2.FlannBasedMatcher()\n",
    "    matches = matcher.match(src_descriptors, des_descriptors)\n",
    "\n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x:x.distance)\n",
    "\n",
    "    # Select top matches\n",
    "    matches = matches[:num_top_matches]\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    src_points = np.float32([src_keypoints[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    des_points = np.float32([des_keypoints[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    if appear == True:\n",
    "        plot_matches(gray_src, gray_des, src_points, des_points)\n",
    "\n",
    "    return src_points, des_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to visualize the matches\n",
    "def plot_matches(src_img, des_img, src_points, des_points):\n",
    "    fig, axes = plt.subplots(1,2,figsize=(10,6))\n",
    "\n",
    "    # draw images\n",
    "    axes[0].imshow(src_img)\n",
    "    axes[1].imshow(des_img)\n",
    "\n",
    "    # draw matches\n",
    "    for src_point, des_point in zip(src_points, des_points):\n",
    "        kp1, kp2 = src_point[0], des_point[0]\n",
    "\n",
    "        con = ConnectionPatch(xyA=kp1, coordsA=axes[0].transData,\n",
    "                              xyB=kp2, coordsB=axes[1].transData, color='r')\n",
    "        fig.add_artist(con)\n",
    "        axes[0].plot(*kp1, color='r', marker='x')\n",
    "        axes[1].plot(*kp2, color='r', marker='x')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending and Compositing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warpTwoImages(img_src, img_des, H, appear=False):\n",
    "    # Warp the source image to align with the destination image \n",
    "    h1,w1 = img_src.shape[:2]\n",
    "    h2,w2 = img_des.shape[:2]\n",
    "\n",
    "    pts1 = np.float32([[0,0],[0,h1],[w1,h1],[w1,0]]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([[0,0],[0,h2],[w2,h2],[w2,0]]).reshape(-1,1,2)\n",
    "\n",
    "    pts2_ = cv2.perspectiveTransform(pts2, H)\n",
    "    pts = np.concatenate((pts1, pts2_), axis=0)\n",
    "\n",
    "    [xmin, ymin] = np.int32(pts.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(pts.max(axis=0).ravel() + 0.5)\n",
    "    t = [-xmin,-ymin]\n",
    "    Ht = np.array([[1,0,t[0]],[0,1,t[1]],[0,0,1]]) # translate\n",
    "\n",
    "    wrap_src_img = cv2.warpPerspective(img_src, Ht.dot(H), (xmax-xmin, ymax-ymin))\n",
    "    plt.imshow(wrap_src_img)\n",
    "    plt.title('Wrap')\n",
    "    plt.show()\n",
    "\n",
    "    # Combine the two images into a panorama\n",
    "    wrap_src_img[t[1]:h2+t[1],t[0]:w2+t[0]] = img_des\n",
    "    \n",
    "    if appear == True:\n",
    "        plt.imshow(wrap_src_img)\n",
    "        plt.title('Panorama result')\n",
    "        plt.show()\n",
    "\n",
    "    return wrap_src_img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stitching_images_harris_test(img_src, img_des):\n",
    "    # Normalize values, change data type\n",
    "    imgs_norm_uint8 = nomalize_images([img_src, img_des], title=\"Nomalized images\")\n",
    "    \n",
    "    # Grayscale images\n",
    "    gray_imgs = to_gray_images(imgs_norm_uint8) \n",
    "\n",
    "    # Harris conner detector\n",
    "    keypoints = harris_coner_detector(gray_imgs,\n",
    "                                    blockSize=2, \n",
    "                                    ksize=3,\n",
    "                                    k_harris_parameter=0.04, \n",
    "                                    threshold=0.01,\n",
    "                                    appear=False)\n",
    "\n",
    "    # Matching images, because there are only two images for testing => only one patched points\n",
    "    src_points, des_points = matching_images(gray_imgs, keypoints,\n",
    "                                                num_top_matches=50,\n",
    "                                                appear=False)\n",
    "\n",
    "    # Compute homography matrix using RANSAC\n",
    "    H, mask = cv2.findHomography(src_points, des_points, cv2.RANSAC, 10.0)\n",
    "\n",
    "    # Find the size of new panorama\n",
    "    panorama = warpTwoImages(img_src, img_des, H, appear=True)\n",
    "\n",
    "    return panorama\n",
    "\n",
    "# HARRIS_IMAGE_RESULT = stitching_images_harris_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_panorama():\n",
    "    # Load images. CV2 reads in BGR format\n",
    "    imgs = data_loader('data/yard/*.png')[:2]\n",
    "\n",
    "    firs_images = imgs[0]\n",
    "    for i in range(1, len(imgs)):\n",
    "        new_firs_images = stitching_images_harris_test(firs_images, imgs[i])\n",
    "        \n",
    "        firs_images = new_firs_images.copy()  \n",
    "    return firs_images\n",
    "\n",
    "panorama = multi_panorama()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General code structure\n",
    "\n",
    "def stitching_images(img_src: np.array, img_des: np.array):\n",
    "    gd_src = rgb2gray(img_src)\n",
    "    gd_des = rgb2gray(img_des)\n",
    "\n",
    "    ### TODO: Harris conner detecting and the patches - areas around conners\n",
    "\n",
    "    ### TODO: SIFT computing the feature description by gradient of each patch\n",
    "\n",
    "    ### TODO: Compute distance matrix based on SIFT of each patch\n",
    "\n",
    "    ### TODO: Match each patch with from distance matrix above\n",
    "\n",
    "    ### TODO: Run RANSAC to remove the outline pair of patch\n",
    "\n",
    "    ### TODO: Composite two images into one\n",
    "\n",
    "    ### TODO: Normalize the images (e.g. have same brightness levels)\n",
    "\n",
    "    return [combined_image, Harris, num_inliers, residual]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
