{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, size=(10, 6), title=\"\"):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=size)\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(image)\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path: str, appear: bool = False, show_size=(10, 6), title: str = \"\"):\n",
    "    image_paths = sorted(glob.glob(path))\n",
    "    images = [cv2.imread(path, cv2.IMREAD_UNCHANGED) for path in image_paths]\n",
    "    if appear:\n",
    "        show_images(images, show_size, title)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(images, appear: bool = False, title: str = \"\"):\n",
    "    normal_images = [(image / np.max(image) * 255).astype(np.uint8) for image in images]\n",
    "    if appear:\n",
    "        show_images(normal_images, title=title)\n",
    "    return normal_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gray_images(images, appear=False, title: str = \"\"):\n",
    "    gray_images = [cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
    "                   for image in images]\n",
    "    if appear:\n",
    "        show_images(gray_images, title=title)\n",
    "    return gray_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_corner_detector(images: list, block_size: int, k_size: int, k_harris_parameter=0.04, threshold=0.01,\n",
    "                           appear=False):\n",
    "    key_points = []\n",
    "    for img in images:\n",
    "        corner = cv2.cornerHarris(img, block_size, k_size, k_harris_parameter)\n",
    "        corner = cv2.dilate(corner, None)\n",
    "\n",
    "        points = np.argwhere(corner > threshold * corner.max())\n",
    "        keypoint = [cv2.KeyPoint(float(x[1]), float(x[0]), 1) for x in points]\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "\n",
    "    if appear:\n",
    "        plot_key_points(images, key_points)\n",
    "\n",
    "    return key_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_key_points(images, key_points):\n",
    "    _, axes = plt.subplots(1, len(images), figsize=(10, 6))\n",
    "    for idx, image in enumerate(images):\n",
    "        points = np.array([list(keypoint.pt) for keypoint in key_points[idx]])\n",
    "        axes[idx].imshow(image)\n",
    "        axes[idx].scatter(points[:, 0], points[:, 1], marker='x', color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT_description(gray_images, key_points):\n",
    "    detector = cv2.SIFT_create()\n",
    "\n",
    "    descriptors = []\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint, descriptor = detector.compute(img, key_points[idx])\n",
    "\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SUFR_description(gray_images):\n",
    "    detector = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "    key_points = []\n",
    "    descriptors = []\n",
    "\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint = detector.detect(img, None)\n",
    "        keypoint, descriptor = detector.compute(img, keypoint)\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ORB_description(gray_images):\n",
    "    detector = cv2.ORB_create()\n",
    "\n",
    "    key_points = []\n",
    "    descriptors = []\n",
    "\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint, descriptor = detector.detectAndCompute(img, None)\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KAZE_description(gray_images):\n",
    "    detector = cv2.KAZE_create()\n",
    "\n",
    "    key_points = []\n",
    "    descriptors = []\n",
    "\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint = detector.detect(img, None)\n",
    "        keypoint, descriptor = detector.compute(img, keypoint)\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FREAK_description(gray_images):\n",
    "    STAR = cv2.xfeatures2d.StarDetector_create()\n",
    "    detector = cv2.xfeatures2d.FREAK_create()\n",
    "\n",
    "    key_points = []\n",
    "    descriptors = []\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint = STAR.detect(img, None)\n",
    "        keypoint, descriptor = detector.compute(img, keypoint)\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BRISK_description(gray_images):\n",
    "    detector = cv2.BRISK_create()\n",
    "\n",
    "    key_points = []\n",
    "    descriptors = []\n",
    "\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint = detector.detect(img, None)\n",
    "        keypoint, descriptor = detector.compute(img, keypoint)\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BRIEF_description(gray_images):\n",
    "    STAR = cv2.xfeatures2d.StarDetector_create()\n",
    "    detector = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "    key_points = []\n",
    "    descriptors = []\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint = STAR.detect(img, None)\n",
    "        keypoint, descriptor = detector.compute(img, keypoint)\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AKAZE_description(gray_images):\n",
    "    detector = cv2.AKAZE_create()\n",
    "\n",
    "    key_points = []\n",
    "    descriptors = []\n",
    "\n",
    "    for idx, img in enumerate(gray_images):\n",
    "        keypoint = detector.detect(img, None)\n",
    "        keypoint, descriptor = detector.compute(img, keypoint)\n",
    "\n",
    "        key_points.append(keypoint)\n",
    "        descriptors.append(descriptor)\n",
    "\n",
    "    return key_points, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_conner(imgs, key_points, descriptors, type_match=\"original\", num_top_matches=50, ratio_thresh=0.3,\n",
    "                    appear=False, method='SIFT'):\n",
    "    #src image\n",
    "    src_key_points = key_points[0]\n",
    "    src_descriptors = descriptors[0]\n",
    "\n",
    "    # des image\n",
    "    des_key_points = key_points[1]\n",
    "    des_descriptors = descriptors[1]\n",
    "\n",
    "    # Match descriptors using FLANN matcher\n",
    "    if method in [\"ORB\", \"FREAK\", \"BRISK\", \"BRIEF\", \"AKAZE\"]:\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        type_match = \"original\"\n",
    "    else:\n",
    "        matcher = cv2.FlannBasedMatcher()\n",
    "\n",
    "    if type_match == \"original\":\n",
    "        matches = matcher.match(src_descriptors, des_descriptors)\n",
    "\n",
    "        # Sort matches by distance\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Select top matches\n",
    "        matches = matches[:num_top_matches]\n",
    "\n",
    "    if type_match == \"knn\":\n",
    "        knn_matches = matcher.knnMatch(src_descriptors, des_descriptors, 2)\n",
    "\n",
    "        # Filter matches using the Lowe's ratio test\n",
    "        ratio_thresh = ratio_thresh\n",
    "        matches = []\n",
    "        for m, n in knn_matches:\n",
    "            if m.distance < ratio_thresh * n.distance:\n",
    "                matches.append(m)\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    src_points = np.float32([src_key_points[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    des_points = np.float32([des_key_points[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    if appear:\n",
    "        plot_matches(imgs[0], imgs[1], src_points, des_points)\n",
    "\n",
    "    return src_points, des_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to visualize the matches\n",
    "def plot_matches(src_img, des_img, src_points, des_points):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "    # draw images\n",
    "    axes[0].imshow(src_img)\n",
    "    axes[1].imshow(des_img)\n",
    "\n",
    "    # draw matches\n",
    "    for src_point, des_point in zip(src_points, des_points):\n",
    "        kp1, kp2 = src_point[0], des_point[0]\n",
    "\n",
    "        con = ConnectionPatch(xyA=kp1, coordsA=axes[0].transData,\n",
    "                              xyB=kp2, coordsB=axes[1].transData, color='r')\n",
    "        fig.add_artist(con)\n",
    "        axes[0].plot(*kp1, color='r', marker='x')\n",
    "        axes[1].plot(*kp2, color='r', marker='x')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending and Compositing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def warpTwoImages(img_src, img_des, H, appear=False):\n",
    "    # Warp the source image to align with the destination image \n",
    "    h1, w1 = img_src.shape[:2]\n",
    "    h2, w2 = img_des.shape[:2]\n",
    "\n",
    "    pts1 = np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2)\n",
    "    pts2 = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)\n",
    "\n",
    "    pts2_ = cv2.perspectiveTransform(pts2, H)\n",
    "    pts = np.concatenate((pts1, pts2_), axis=0)\n",
    "\n",
    "    [xmin, ymin] = np.int32(pts.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(pts.max(axis=0).ravel() + 0.5)\n",
    "    t = [-xmin, -ymin]\n",
    "    Ht = np.array([[1, 0, t[0]], [0, 1, t[1]], [0, 0, 1]])  # translate\n",
    "\n",
    "    wrap_src_img = cv2.warpPerspective(img_src, Ht.dot(H), (xmax - xmin, ymax - ymin))\n",
    "\n",
    "    # Combine the two images into a panorama\n",
    "    wrap_src_img[t[1]:h2 + t[1], t[0]:w2 + t[0]] = img_des\n",
    "\n",
    "    if appear:\n",
    "        plt.imshow(wrap_src_img)\n",
    "        # plt.title('Panorama result')\n",
    "        plt.show()\n",
    "\n",
    "    return wrap_src_img  #%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stitching_images(img_src, img_des, method='SIFT'):\n",
    "    ## Normalize values, change data type\n",
    "    images_norm_uint8 = normalize_images([img_src, img_des], appear=False)\n",
    "\n",
    "    ## Grayscale images\n",
    "    gray_images = to_gray_images(images_norm_uint8, appear=False)\n",
    "\n",
    "    ## Harris conner detector\n",
    "    key_points = harris_corner_detector(gray_images,\n",
    "                                        block_size=2,\n",
    "                                        k_size=3,\n",
    "                                        k_harris_parameter=0.04,\n",
    "                                        threshold=0.05,\n",
    "                                        appear=False)\n",
    "\n",
    "    ## Description of each corner\n",
    "    if method == 'SIFT':\n",
    "        key_points, descriptors = SIFT_description(gray_images, key_points)\n",
    "\n",
    "    if method == 'SURF':\n",
    "        # SURF is the same idea as SIFT, but it's patented\n",
    "        # to run it, we need old version of opencv < 3.4.\n",
    "        key_points, descriptors = SUFR_description(gray_images)\n",
    "\n",
    "    if method == 'ORB':\n",
    "        key_points, descriptors = ORB_description(gray_images)\n",
    "\n",
    "    if method == 'KAZE':\n",
    "        key_points, descriptors = KAZE_description(gray_images)\n",
    "\n",
    "    if method == 'FREAK':\n",
    "        key_points, descriptors = FREAK_description(gray_images)\n",
    "\n",
    "    if method == 'BRISK':\n",
    "        key_points, descriptors = BRISK_description(gray_images)\n",
    "\n",
    "    if method == 'BRIEF':\n",
    "        key_points, descriptors = BRIEF_description(gray_images)\n",
    "\n",
    "    if method == 'AKAZE':\n",
    "        key_points, descriptors = AKAZE_description(gray_images)\n",
    "\n",
    "    ## Matching corner\n",
    "    src_points, des_points = matching_conner([img_src, img_des],\n",
    "                                             key_points, descriptors,\n",
    "                                             type_match=\"original\",\n",
    "                                             num_top_matches=50,\n",
    "                                             ratio_thresh=0.45,\n",
    "                                             appear=False,\n",
    "                                             method=method)\n",
    "\n",
    "    ## Compute homography matrix using RANSAC\n",
    "    H, mask = cv2.findHomography(src_points, des_points, cv2.RANSAC, 2.0)\n",
    "\n",
    "    ## Find the size of new panorama\n",
    "    panorama = warpTwoImages(img_src, img_des, H, appear=False)\n",
    "\n",
    "    return panorama\n",
    "\n",
    "# HARRIS_IMAGE_RESULT = stitching_images_harris_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average processing time: 0.44 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nAverage processing times:\\nAKAZE: 0.40s\\nBRIEF: 0.25s\\nBRISK: 0.41s\\nFREAK: 0.27s\\nKAZE: 0.94s\\nORB: 0.25s\\nSIFT: \\n'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_panorama():\n",
    "    # Load images. CV2 reads in BGR format\n",
    "    images = data_loader('data/xue-mountain/*.jpg')[0:2]\n",
    "\n",
    "    firs_images = images[0]\n",
    "    for i in range(1, len(images)):\n",
    "        # method = ['SIFT', 'SURF', 'ORB', 'KAZE', 'FREAK', 'BRISK', 'BRIEF', 'AKAZE']\n",
    "        method = 'SIFT'\n",
    "\n",
    "        new_firs_images = stitching_images(firs_images, images[i], method=method)\n",
    "\n",
    "        firs_images = new_firs_images.copy()\n",
    "\n",
    "    # Resize final stitched image to match dimensions of first image\n",
    "    firs_images = cv2.resize(firs_images, (images[0].shape[1], images[0].shape[0]))\n",
    "\n",
    "    # Calculate MSE and PSNR of the final stitched image\n",
    "    mse_val = np.mean((firs_images - images[0]) ** 2)\n",
    "    psnr_val = cv2.PSNR(firs_images, images[0])\n",
    "    # print(f\"Final image: MSE = {mse_val:.2f}, PSNR = {psnr_val:.2f}\")\n",
    "\n",
    "    return firs_images\n",
    "\n",
    "\"\"\"\n",
    "# uncomment to measure processing times\n",
    "from timeit import timeit\n",
    "\n",
    "TIME_LOOP = 50\n",
    "processing_time_result = timeit(\n",
    "    lambda: multi_panorama(), number=TIME_LOOP\n",
    ")\n",
    "print(f'Average processing time: {processing_time_result / TIME_LOOP:.2f} seconds')\n",
    "\"\"\"\n",
    "PANORAMA = multi_panorama()\n",
    "\n",
    "\"\"\"\n",
    "Average processing times:\n",
    "AKAZE: 0.40s\n",
    "BRIEF: 0.25s\n",
    "BRISK: 0.41s\n",
    "FREAK: 0.27s\n",
    "KAZE: 0.94s\n",
    "ORB: 0.25s\n",
    "SIFT: 0.44s\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
